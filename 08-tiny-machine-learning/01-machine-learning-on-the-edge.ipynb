{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning on the Edge\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/riccardoberta/machine-learning/blob/master/08-tiny-machine-learning/01-xxx.ipynb)\n",
    "\n",
    "\n",
    "It became clear that there was a whole new class of products emerging, with the\n",
    "key characteristics that they used ML to make sense of noisy sensor data, could run using a battery (or energy harvesting) for years, and cost only a dollar or two. The \"peel-and-stick sensors\" idea for devices that required no battery changes and could be applied anywhere in an environment and forgotten. Making these products real required ways to turn raw sensor data into actionable information locally, on the device itself, since the energy costs of transmitting streams anywhere have proved to be inherently too high to be practical.\n",
    "\n",
    "As an example, the \"OK Google\" feature of Android is running a neural network that is just 14 KB in size! It has to be so small because it is running on the digital signal processor (DSP) present in the smarphones, continuously listening for the \"OK Google\" wake words, and thes DSP had only tens of kilobytes of RAM and flash memory. The use the DSPs for that job is needed because the main CPU is powered off to conserve battery, and the specialized chip use only a few milliwatts of power.\n",
    "\n",
    "Raspberry Pi or NVIDIA Jetson boards are fantastic devices, but even the smallest is similar to a mobile phone main CPU and draws hundreds of milliwatts. Keeping one running even for a few days requires a battery similar to a smartphone’s, making it difficult to build truly untethered experiences.\n",
    "Jetson is based on a powerful GPU, and it use up to 12 watts of power when running at full speed, so it’s even more difficult to use without a large external power supply. \n",
    "\n",
    "This is usually not a problem in automotive or robotics applications, since the\n",
    "mechanical parts demand a large power source themselves, but it does make it tough to use these platforms for the kinds of products which need to operate without a wired power supply. \n",
    "\n",
    "Another important characteristic is cost. The cheapest Raspberry Pi Zero is some dollars for makers, but it is extremely difficult to buy in large numbers. By contrast, the cheapest 32-bit microcontrollers cost much less than a dollar. This low price made it possible for manufacturers to replace traditional analog or electromechanical control circuits with software-defined alternatives for everything from toys to washing machines. We can use the ubiquity of microcontrollers in these devices to introduce artificial intelligence as a software update, without requiring a lot of changes to existing designs.\n",
    "\n",
    "\n",
    "1. [](#) \n",
    "2. [](#)\n",
    "3. [](#)\n",
    "4. [](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep networks are increasingly used for applications at the edge. Devices at the edge\n",
    "typically have lower compute capabilities and are constrained in memory and power\n",
    "consumption. It is also necessary to reduce the amount of communication to the cloud\n",
    "for transferring models to the device to save on power and reduce network connectivity\n",
    "requirements. Therefore, there is a pressing need for techniques to optimize models for\n",
    "reduced model size, faster inference and lower power consumption.\n",
    "\n",
    "There is extensive research on this topic with several approaches being considered:\n",
    "One approach is to build efficient models from the ground up [1],[2] and [3]. Another\n",
    "technique is to reduce the model size by applying quantization, pruning and compression techniques [4], [5] and [6]. Faster inference has been achieved by having efficient\n",
    "kernels for computation in reduced precision like GEMMLOWP [7], Intel MKL-DNN\n",
    "[8] , ARM CMSIS [9], Qualcomm SNPE [10], Nvidia TensorRT [11] and custom hardware for fast inference [12], [13] and [14].\n",
    "\n",
    "[1] M. Sandler, A. G. Howard, M. Zhu, A. Zhmoginov, and L. Chen, “Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation,” 2018.\n",
    "[2] A. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, “MobileNets: Efficient Convolutional Neural Networks\n",
    "for Mobile Vision Applications,” Apr. 2017.\n",
    "[3] F. N. Iandola, M. W. Moskewicz, K. Ashraf, S. Han, W. J. Dally, and K. Keutzer,\n",
    "“Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <1mb\n",
    "model size,” CoRR, vol. abs/1602.07360, 2016.\n",
    "[4] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and\n",
    "D. Kalenichenko, “Quantization and Training of Neural Networks for Efficient\n",
    "Integer-Arithmetic-Only Inference,” Dec. 2017.\n",
    "[5] M. Courbariaux, Y. Bengio, and J. David, “Binaryconnect: Training deep neural\n",
    "networks with binary weights during propagations,” 2015.\n",
    "[6] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding,” CoRR,\n",
    "vol. abs/1510.00149, 2015.\n",
    "[7] GEMMLOWP, “Gemmlowp: a small self-contained low-precision GEMM library.” https://github.com/google/gemmlowp.\n",
    "[8] Intel(R) MKL-DNN, “Intel(R) Math Kernel Library for Deep Neural Networks.”\n",
    "https://intel.github.io/mkl-dnn/index.html.\n",
    "[9] ARM, “Arm cmsis nn software library.”\n",
    "http://arm-software.github.io/CMSIS 5/NN/html/index.html.\n",
    "[10] Qualcomm onQ blog, “How can Snapdragon 845’s new AI boost your smartphone’s IQ?.”\n",
    "https://www.qualcomm.com/news/onq/2018/02/01/how-can-snapdragon-845snew-ai-boost-your-smartphones-iq.\n",
    "[11] Nvidia, “8 bit inference with TensorRT.”\n",
    "http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inferencewith-tensorrt.pdf.\n",
    "[12] V. Sze, Y. Chen, T. Yang, and J. S. Emer, “Efficient processing of deep neural\n",
    "networks: A tutorial and survey,” CoRR, vol. abs/1703.09039, 2017.\n",
    "32\n",
    "[13] Nvidia, “The nvidia deep learning accelerator.” http://nvdla.org/.\n",
    "[14] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, and W. J. Dally, “EIE:\n",
    "efficient inference engine on compressed deep neural network,” 2016.\n",
    "[15] “Android Neural Network API.”\n",
    "https://developer.android.com/ndk/guides/neuralnetworks/#quantized tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedded Devices\n",
    "\n",
    "Until a few years ago, embedded deviced had been 8-bit devices and used obscure and proprietary toolchains. A big step forward came when Arduino introduced a user-friendly IDE and standardized hardware. Since then, 32-bit CPUs have\n",
    "become the standard, largely thanks to **Arm Cortex-M** series of chips. \n",
    "\n",
    "However, embedded devices still come with some **resource constraints**: often they have only a few hundred KBytes of RAM and a similar amounts of flash memory for persistent storage; the clock speed is just tens of megahertz and they don't have full Linux OS; if there is an OS, it not provide all of the standard C library functions and in many case it is avoided the use of dynamic memory allocation functions (because they’re designed to be reliable and long-running and it’s extremely difficult to ensure that if you have a fragmented heap); finally it might be tricky to use a debugger since the interfaces to access the chip are very specialized.\n",
    "\n",
    "There were also some nice features in the embedded development: having a system with no other processes to interrupt our program can make building a mental model of what’s happening very simple, and the straightforward nature of a processor without branch prediction or instruction pipelining makes manual assembly optimization a lot easier than on more complex CPUs. \n",
    "\n",
    "We need an embedded development board to test our programs on, for example the [SparkFun Edge board](https://www.sparkfun.com/products/15170). However, all\n",
    "of the projects are adaptable to other devices if you can capture the sensor data in the formats needed.\n",
    "\n",
    "Finally, all of the examples are based around the [TensorFlow Lite for Microcontrollers](https://github.com/tensorflow/tflite-micro), which is a port of TensorFlow Lite designed to run machine learning models on DSPs, microcontrollers and other devices with limited memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple model\n",
    "\n",
    "We will train a simple model that can take a value and predict its sine and we’ll use the output of our model to control the timing of some flashing LEDs.\n",
    "In a realworld application, if we need the sine of x, we can just calculate it directly. However, by training a model to approximate the result, we can show the basics of the etire process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "math_differential_calculus",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "8aeb84091b1f1fb8d8b9efbf1e96a552fa0144c39bfbc7f744113ad2216f701d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
