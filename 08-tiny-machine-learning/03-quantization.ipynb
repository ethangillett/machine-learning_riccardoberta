{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Quantization\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/riccardoberta/machine-learning/blob/master/08-tiny-machine-learning/03-quantization.ipynb)\n",
    "\n",
    "Running a DNN on edge devices (with low power and limited memory) is often a challenge due to the huge memory requirements to store the input data, weight parameters and activations of the network as well as the massive amount of computations required for the calculation of the weighted sums of the neurons inputs. \n",
    "\n",
    "To tackle this problem, a possible solution is to perform **Quantization** ([B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko, \"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\", 2017](https://arxiv.org/abs/1712.05877)). It isn't something new, it has been around for many years and have been extensively since the creation of digital electronics. In digital signal processing, quantization usually refers to converting continuous signal (like light or sound) into discrete digital number. In deep learning, quantization means converting signed single precision floating point (float32) to lower precision number format such as unsigned 8-bit integer (uint8).  Although quantization can reduce memory, it is not mainly to save storage but rather to speed up the inference. This can happen for two reasons: deep neural networks have a lot of parameters (weights) and loading them from memory can become the bottleneck rather than the actual computation, so is moving the activations back the the main memor; moreover fixed point computation is usually faster than the floating point counterpart. \n",
    "\n",
    "Quantization mapping\n",
    "1. [Quantization mapping](#Quantization-mapping) \n",
    "2. [Quantized Matrix Multiplication](#Quantized-Matrix-Multiplication) \n",
    "3. [Quantized Deep Learning Layers](#Quantized-Deep-Learning-Layers) \n",
    "4. [Neural Networks Integer Quantization Modes](#Neural-Networks-Integer-Quantization-Modes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization mapping\n",
    "\n",
    "Quantization maps a floating point value $x \\in [\\alpha, \\beta]$ to a b-bit integer  $x_q \\in [\\alpha_q, \\beta_q]$. Mathematically, the quantization process is defined as:\n",
    "\n",
    "$\\begin{align}\n",
    "x_q = \\text{round}(\\frac{x}{c} - d)\n",
    "\\end{align}$\n",
    "\n",
    "and the de-quantization process is defined as:\n",
    "\n",
    "$\\begin{align}\n",
    "x = c(x_q + d)\n",
    "\\end{align}$\n",
    "\n",
    "where $c$ and $d$ are parameters which can be derived in order to assure that $\\alpha_q$ maps to $\\alpha$ and $\\beta_q$ maps to $\\beta$. \n",
    "\n",
    "<img src=\"./images/quantization-mapping.png\" width=\"500\"> \n",
    "\n",
    "So we would just have to solve a linear system:\n",
    "\n",
    "$\\begin{align}\n",
    "\\alpha = c(\\alpha_q + d)\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "\\beta = c(\\beta_q + d)\n",
    "\\end{align}$\n",
    "\n",
    "The solution is:\n",
    "\n",
    "$\\begin{align}\n",
    "c = \\frac{\\beta - \\alpha}{\\beta_q - \\alpha_q}\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "d = \\frac{\\alpha \\beta_q - \\beta \\alpha_q}{\\beta - \\alpha}\n",
    "\\end{align}$\n",
    "\n",
    "In practice, we would have to ensure that 0 in floating point is represented exactly with no error after quantization. Mathematichally, we need to ensure:\n",
    "\n",
    "$\\begin{align}\n",
    "x_q = \\text{round}(\\frac{0}{c} - d) = -\\text{round}(d)\n",
    "\\end{align}$\n",
    "\n",
    "This means that:\n",
    "\n",
    "$\\begin{align}\n",
    "d = \\text{round}(d) = \\text{round}(\\frac{\\alpha \\beta_q - \\beta \\alpha_q}{\\beta - \\alpha})\n",
    "\\end{align}$\n",
    "\n",
    "By convention, we denote $c$ as the **scale** $s$ and $−d$ as the **zero point** z. To summarize, the quantization and de-quantization processes are defined as:\n",
    "\n",
    "$\\begin{align}\n",
    "x_q = \\text{round}(\\frac{x}{s} + z)\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "x = s(x_q - z)\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "s = \\frac{\\beta - \\alpha}{\\beta_q - \\alpha_q}\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "z = \\text{round}(\\frac{\\beta \\alpha_q - \\alpha \\beta_q }{\\beta - \\alpha})\n",
    "\\end{align}$\n",
    "\n",
    "If the integer type is **signed INTb** then:\n",
    "\n",
    "$\\begin{align}\n",
    "(\\alpha_q, \\beta_q) = (-2^{b-1}, 2^{b-1}-1)\n",
    "\\end{align}$\n",
    "\n",
    "Else, if the integer type is **unsigned UINTb** then:\n",
    "\n",
    "$\\begin{align}\n",
    "(\\alpha_q, \\beta_q) = (0, 2^b-1)\n",
    "\\end{align}$\n",
    "\n",
    "In practice, the quantization process will have chance to have $x$ that is outside the range of $[\\alpha, \\beta]$, thus the quantized value $x_q$  will also be outside the range of $[\\alpha_q, \\beta_q]$ and we need to clip the values that are outside the range. More concretely, the quantization process will have an additional clip step: \n",
    "\n",
    "$\\begin{align}\n",
    "x_q = \\text{clip}(\\text{round}(\\frac{x}{s} + z), \\alpha_q, \\beta_q)\n",
    "\\end{align}$\n",
    "\n",
    "There are other ways of quantization mapping, potentially even non-linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def quantization(x, s, z, alpha_q=-128, beta_q=127):\n",
    "    x_q = np.round(1 / s * x + z, decimals=0)\n",
    "    x_q = np.clip(x_q, a_min=alpha_q, a_max=beta_q)\n",
    "    return x_q\n",
    "\n",
    "def dequantization(x_q, s, z):\n",
    "    x = s * (x_q - z)\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def generate_quantization_constants(alpha, beta, alpha_q=-128, beta_q=127):\n",
    "    s = (beta - alpha) / (beta_q - alpha_q)\n",
    "    z = int((beta * alpha_q - alpha * beta_q) / (beta - alpha))\n",
    "    return s, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = -100\n",
    "beta = 100\n",
    "\n",
    "alpha_q = -10\n",
    "beta_q = 10\n",
    "\n",
    "s,z = generate_quantization_constants(alpha, beta, alpha_q, beta_q)\n",
    "\n",
    "x = np.arange (alpha-50, beta+50, 0.1)\n",
    "x_q = quantization(x, s, z, alpha_q, beta_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, x_q,color='r')\n",
    "ax.set_xlabel(\"floating point\")\n",
    "ax.set_ylabel(\"quantized integer\")\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantized Matrix Multiplication\n",
    "\n",
    "Suppose we have to perform the tipycal matrix multiplication of a neuron:\n",
    "\n",
    "$\\begin{align}\n",
    "Y = X W + b\n",
    "\\end{align}$\n",
    "\n",
    "where $X \\in R^{m \\times p}$, $W \\in R^{p \\times n}$ and $b \\in R^n$ resulting in $Y \\in R^{m \\times n}$\n",
    "\n",
    "We would need to do $p$ floating number multiplications and $p$ floating number additions to compute one single entry in $Y$:\n",
    "\n",
    "$\\begin{align}\n",
    "Y_{i,j} = b_j + \\sum\\limits_{k=1}^{p}{X_{i,k}W_{k,j}} \n",
    "\\end{align}$\n",
    "\n",
    "To complete the full matrix multiplication, given there are ($m \\cdot n$) entries in $Y$, we would need to do ($m\\cdot p\\cdot n$) floating number multiplications and ($m\\cdot p\\cdot n$) floating number additions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we complete the same matrix multiplication using quantized values? Here we apply the de-quantization equation:\n",
    "\n",
    "$\\begin{align}\n",
    "Y_{i,j} = b_j + \\sum\\limits_{k=1}^{p}{X_{i,k}W_{k,j}}=\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "= s_b(b_{q,j}-z_b) + \\sum\\limits_{k=1}^{p}{s_X(X_{q,i,k}-z_X)s_W(W_{q,k,j}-z_W)}=\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "= s_b(b_{q,j}-z_b) + s_X s_W \\sum\\limits_{k=1}^{p}{(X_{q,i,k}-z_X)(W_{q,k,j}-z_W)}=\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "= s_b(b_{q,j}-z_b) + s_X s_W \\left[ (\\sum\\limits_{k=1}^{p}{X_{q,i,k}W_{q,k,j}}) - (z_W \\sum\\limits_{k=1}^{p}{X_{q,i,k}}) - (z_X \\sum\\limits_{k=1}^{p}{W_{q,i,k}}) + p z_X z_W \\right]\n",
    "\\end{align}$\n",
    "\n",
    "where $X_q$, $W_q$ and $b_q$ are the quantized matrix for $X$, $W$ and $b$; $s_X$, $s_W$ and $s_b$ are the scales; and $z_X$, $z_W$ and $z_b$ are the zero point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can express the operation with quantized matrix $Y_q$:\n",
    "\n",
    "$\\begin{align}\n",
    "Y_{i,j} = s_Y(Y_{q,i,j}-z_Y)\n",
    "\\end{align}$\n",
    "\n",
    "therefore,\n",
    "\n",
    "$\\begin{align}\n",
    "Y_{q,i,j} = z_Y + \\frac{s_b}{s_Y}(b_{q,j}-z_b) + \\frac{s_X s_W}{s_Y} \\left[ (\\sum\\limits_{k=1}^{p}{X_{q,i,k}W_{q,k,j}}) - (z_W \\sum\\limits_{k=1}^{p}{X_{q,i,k}}) - (z_X \\sum\\limits_{k=1}^{p}{W_{q,i,k}}) + p z_X z_W \\right]\n",
    "\\end{align}$\n",
    "\n",
    "Notice that several terms are constant during the inference (don't depend on the particular input) and can be computed offline:\n",
    "\n",
    "- $z_Y$\n",
    "- $\\frac{s_b}{s_Y}(b_{q,j}-z_b)$\n",
    "- $z_X \\sum\\limits_{k=1}^{p}{W_{q,i,k}}$\n",
    "- $p z_X z_W$\n",
    "\n",
    "The term\n",
    "\n",
    "$\\begin{align}\n",
    "\\sum\\limits_{k=1}^{p}{X_{q,i,k}W_{q,k,j}}\n",
    "\\end{align}$\n",
    "\n",
    "suggest that we could just do the integer matrix multiplication for $X_q$ and $W_q$. Such integer matrix multiplication could employ special hardware and algorithms, such as [NVIDIA Tensor Core IMMA operations](https://docs.nvidia.com/cuda/ampere-tuning-guide/index.html#tensor-operations), and runs much faster than conventional integer matrix multiplication. Where IMMA stands for **Integer Matrix Multiply and Accumulate** operation.\n",
    "\n",
    "<img src=\"./images/tensor-core-operation.png\" width=\"500\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The product integer matrix could be converted back to floating point matrix. Suppose that $f_q$ is the quantization function, $f_m$ is the quantized matrix multiplication function and $f_d$ is the de-quantization function, then we can have  a sequence of matrix multiplications, likw in the ANN inference, whose inputs and outputs are floating point:\n",
    "\n",
    "$\\begin{align}\n",
    "X_1 = X_0W_0 + b_0\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "X_2 = X_1W_1 + b_1\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "...\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "X_n = X_{n-1}W_{n-1} + b_{n-1}\n",
    "\\end{align}$\n",
    "\n",
    "We can convert the math using the quatized matrices:\n",
    "\n",
    "$\\begin{align}\n",
    "X_{0,q} = f_q(X_0, s_{X_0}, z_{X_0})\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "X_{1,q} = f_m(X_{0,q}, W_{0,q}, b_{0,q})\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "X_{2,q} = f_m(X_{1,q}, W_{1,q}, b_{1,q})\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "...\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "X_{n,q} = f_m(X_{n-1,q}, W_{n-1,q}, b_{n-1,q})\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "X_n = f_d(X_{n,q}, s_{X_n}, z_{X_n})\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we implement the quantization matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantization_matrix_multiplication(X_q, W_q, b_q, s_X, z_X, s_W, z_W, s_b, z_b, s_Y, z_Y):\n",
    "    p = W_q.shape[0]\n",
    "    Y_q = (z_Y + \n",
    "          (s_b / s_Y * (b_q - z_b)).astype(np.int8) +\n",
    "          ((s_X * s_W / s_Y) * (np.matmul(X_q, W_q) - \n",
    "                               z_W * np.sum(X_q, axis=1, keepdims=True) -\n",
    "                               z_X * np.sum(W_q, axis=0, keepdims=True) +\n",
    "                               p * z_X * z_W)).astype(np.int8)).astype(np.int8)\n",
    "\n",
    "    return Y_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make an example comparing the floating point multiplication and the quantized multiplication for some random matices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y:\n",
      "[[ -422.73868   -534.7745   -1484.6167    -473.53763 ]\n",
      " [-1658.9243      19.164497  -763.61664   2194.8906  ]]\n",
      "Expected Y_q:\n",
      "[[-18. -23. -63. -20.]\n",
      " [-71.   1. -32.  93.]]\n",
      "Y_q from Quantized Matrix Multiplication:\n",
      "[[-17 -23 -62 -20]\n",
      " [-69   1 -32  92]]\n",
      "Y from dequantizaion of Y_q:\n",
      "[[ -400.        -541.17645  -1458.8235    -470.58823 ]\n",
      " [-1623.5294      23.529411  -752.94116   2164.7058  ]]\n"
     ]
    }
   ],
   "source": [
    "m = 2\n",
    "p = 3\n",
    "n = 4\n",
    "\n",
    "# X\n",
    "alpha_X = -100.0\n",
    "beta_X = 80.0\n",
    "X = np.random.uniform(low=alpha_X, high=beta_X, size=(m, p)).astype(np.float32)\n",
    "s_X, z_X = generate_quantization_constants(alpha=alpha_X, beta=beta_X)\n",
    "X_q = quantization(x=X, s=s_X, z=z_X)\n",
    "\n",
    "# W\n",
    "alpha_W = -20.0\n",
    "beta_W = 10.0\n",
    "W = np.random.uniform(low=alpha_W, high=beta_W, size=(p, n)).astype(np.float32)\n",
    "s_W, z_W = generate_quantization_constants(alpha=alpha_W, beta=beta_W)\n",
    "W_q = quantization(x=W, s=s_W, z=z_W)\n",
    "\n",
    "# b\n",
    "alpha_b = -500.0\n",
    "beta_b = 500.0\n",
    "s_b, z_b = generate_quantization_constants(alpha=alpha_b, beta=beta_b)\n",
    "b = np.random.uniform(low=alpha_b, high=beta_b, size=(1, n)).astype(np.float32)\n",
    "b_q = quantization(x=b, s=s_b, z=z_b)\n",
    "\n",
    "# Y\n",
    "alpha_Y = -3000.0\n",
    "beta_Y = 3000.0\n",
    "s_Y, z_Y = generate_quantization_constants(alpha=alpha_Y, beta=beta_Y)\n",
    "Y = np.matmul(X, W) + b\n",
    "Y_q_expected = quantization(x=Y, s=s_Y, z=z_Y)\n",
    "\n",
    "print(\"Y:\")\n",
    "print(Y)\n",
    "print(\"Expected Y_q:\")\n",
    "print(Y_q_expected)\n",
    "\n",
    "# Quantized Matrix Multiplication \n",
    "Y_q_from_multiplication = quantization_matrix_multiplication(X_q, W_q, b_q, s_X, z_X, s_W,z_W,s_b,z_b,s_Y,z_Y)\n",
    "Y_from_dequantization = dequantization(Y_q_from_multiplication, s_Y, z_Y)\n",
    "\n",
    "print(\"Y_q from Quantized Matrix Multiplication:\")\n",
    "print(Y_q_from_multiplication)\n",
    "print(\"Y from dequantizaion of Y_q:\")\n",
    "print(Y_from_dequantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the product FP32 matrix computed using quantized matrices is close to the expectation computed using matrices floating point matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantized Deep Learning Layers\n",
    "\n",
    "In addition to matrix multiplications, deep learning models also have non-linear activation layers such as ReLU and other special layers such as batch normalization. So the question becomes how do we deal with these layers in a quantized deep learning model in practice?\n",
    "\n",
    "One trivial solution is to de-quantize the quantized input tensor to these layers, use ordinary floating point computations, and quantize the output tensors. This will work if there are only a few such layers in the model or there is no special implementations to handle these layers in a quantized manner. However, in most of the deep learning models, the number of such layers are not negligible, and using this trivial solution is likely to slow the inference down significantly.\n",
    "\n",
    "The other solution is to deal with each individual special layers carefully. As an example, we can consider the quantized version of the ReLU activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized ReLU\n",
    "\n",
    "We define the activation function ReLU as follows. This might look different from the the conventional ReLU definition, but such definition is more generalized and is convenient for the demonstration of quantized ReLU.\n",
    "\n",
    "$\\begin{align}\n",
    "\\text{ReLU}(x,x_t,y_v,k)= \\Big\\{ \\begin{matrix} y_v  \\enspace \\enspace \\enspace \\enspace \\enspace \\enspace \\enspace \\enspace \\enspace \\enspace \\text{if} \\enspace x \\lt x_t \\\\ y_v + k(x-x_t) \\enspace \\text{if} \\enspace x\\geq x_t \\end{matrix} \n",
    "\\end{align}$\n",
    "\n",
    "The common ReLU is a special case where $x_t=0$, $y_v=0$ and $k=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x, x_t, y_v, k):\n",
    "    x = np.clip(x, a_min=x_t, a_max=None)\n",
    "    y = y_v + k * (x - x_t)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = 0\n",
    "y_v = 0\n",
    "k = 1\n",
    "\n",
    "x = np.arange (-5, 5, 0.1)\n",
    "y = relu(x, x_t, y_v, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y,color='r')\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"ReLU\")\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let’s derive how the quantized ReLU mathematically:\n",
    "\n",
    "$\\begin{align}\n",
    "y = \\text{ReLU}(x,0,0,1)= \\Big\\{ \\begin{matrix} 0 \\enspace \\text{if} \\enspace x \\lt 0 \\\\ x \\enspace \\text{if} \\enspace x\\geq 0 \\end{matrix} =\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "= s_y(y_q - z_y) =\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "= \\text{ReLU}(s_x(x_q-z_x),0,0,1)= \\Big\\{ \\begin{matrix} 0 \\enspace \\enspace \\enspace \\enspace \\enspace \\enspace \\enspace \\enspace \\text{if} \\enspace s_x(x_q-z_x) \\lt 0 \\\\ s_x(x_q-z_x) \\enspace \\text{if} \\enspace s_x(x_q-z_x)\\geq 0 \\end{matrix} =\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    "= \\text{ReLU}(s_x(x_q-z_x),0,0,1)= \\Big\\{ \\begin{matrix} 0 \\enspace \\enspace \\enspace \\enspace \\enspace \\enspace \\enspace \\enspace \\text{if} \\enspace x_q \\lt z_x \\\\ s_x(x_q-z_x) \\enspace \\text{if} \\enspace x_q \\geq z_x \\end{matrix}\n",
    "\\end{align}$\n",
    "\n",
    "therefore,\n",
    "\n",
    "$\\begin{align}\n",
    "y_q = \\Big\\{ \\begin{matrix} z_y \\enspace \\enspace \\enspace \\enspace \\enspace \\enspace \\enspace \\enspace \\enspace \\enspace \\text{if} \\enspace x_q \\lt z_x \\\\ z_y + \\frac{s_x}{s_y} (x_q-z_x) \\enspace \\text{if} \\enspace x_q \\geq z_x \\end{matrix} = \\text{ReLU}(x_q, z_x, z_y, {s_x}/{s_y})\n",
    "\\end{align}$\n",
    "\n",
    "So it becomes apparent that to do the quantized ReLU corresponding to the floating point $y = ReLU(x,0,0,1)$, we just have to do $y_q = ReLU(x_q, z_x, z_y,  {s_x}/{s_y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantization_relu(x_q, s_x, z_x, s_y, z_y):\n",
    "    y_q = relu(x_q, z_x, z_y, s_x/s_y).astype(np.uint8)\n",
    "    return y_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_x = -60.0\n",
    "beta_x = 60.0\n",
    "s_x, z_x = generate_quantization_constants(alpha_x, beta_x)\n",
    "x = np.arange (-5, 5, 0.1)\n",
    "x_q = quantization(x, s_x, z_x)\n",
    "\n",
    "alpha_y = -100.0\n",
    "beta_y = 100.0\n",
    "s_y, z_y = generate_quantization_constants(alpha_y, beta_y)\n",
    "\n",
    "y_q = quantization_relu(x_q, s_x, z_x, s_y, z_y)\n",
    "y = dequantization(y_q, s_y, z_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y_q,color='r')\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"Quantized ReLU\")\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make some simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[[ 53.945545 -14.557505  25.265274  14.182593]\n",
      " [-51.783535  25.961891 -22.418495  11.479239]]\n",
      "X_q:\n",
      "[[ 115.  -31.   54.   30.]\n",
      " [-110.   55.  -48.   24.]]\n",
      "Y:\n",
      "[[53.945545  0.       25.265274 14.182593]\n",
      " [ 0.       25.961891  0.       11.479239]]\n",
      "Y_q:\n",
      "[[69.  0. 32. 18.]\n",
      " [ 0. 33.  0. 15.]]\n",
      "Y_q from Quantized ReLU:\n",
      "[[69  0 32 18]\n",
      " [ 0 33  0 14]]\n",
      "Y from Quantized ReLU:\n",
      "[[54.117645  0.       25.09804  14.117647]\n",
      " [ 0.       25.882353  0.       10.980392]]\n"
     ]
    }
   ],
   "source": [
    "# Random matrices\n",
    "m = 2\n",
    "n = 4\n",
    "\n",
    "alpha_X = -60.0\n",
    "beta_X = 60.0\n",
    "s_X, z_X = generate_quantization_constants(alpha_X, beta_X)\n",
    "X = np.random.uniform(alpha_X, beta_X, size=(m, n)).astype(np.float32)\n",
    "X_q = quantization(X, s_X, z_X)\n",
    "\n",
    "alpha_Y = -100\n",
    "beta_Y = 100\n",
    "s_Y, z_Y = generate_quantization_constants(alpha_Y, beta_Y)\n",
    "Y = relu(X, 0, 0, 1)\n",
    "Y_q = quantization(Y, s_Y, z_Y)\n",
    "\n",
    "print(\"X:\")\n",
    "print(X)\n",
    "print(\"X_q:\")\n",
    "print(X_q)\n",
    "\n",
    "print(\"Y:\")\n",
    "print(Y)\n",
    "print(\"Y_q:\")\n",
    "print(Y_q)\n",
    "\n",
    "Y_q_from_quantized_ReLU = quantization_relu(X_q,s_X,z_X,s_Y,z_Y)\n",
    "Y_from_quantized_ReLU = dequantization(Y_q_from_quantized_ReLU, s_Y, z_Y)\n",
    "\n",
    "print(\"Y_q from Quantized ReLU:\")\n",
    "print(Y_q_from_quantized_ReLU)\n",
    "print(\"Y from Quantized ReLU:\")\n",
    "print(Y_from_quantized_ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the FP32 ReLU activations computed using quantized matrices is close to the one computed using floating point matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Integer Quantization Modes\n",
    "\n",
    "There are generally two modes for appliyng quantization to neural networks: **post-training quantization** and **quantization aware training**. In order to introduce both of them, we build a Keras Model for MNIST classification and we will use the quantization feature provided by [TensorFlow Lite model optimization](https://www.tensorflow.org/lite/performance/model_optimization?hl=en)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "import datetime, os\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset as train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(60000, 784).astype('float32')\n",
    "x_test = x_test.reshape(10000, 784).astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "num_classes = 10\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dropout (Dropout)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              785000    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                10010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,796,010\n",
      "Trainable params: 1,796,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 15:10:40.924920: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import models, layers\n",
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dropout(0.2,input_shape=(784,)))\n",
    "model.add(layers.Dense(1000, kernel_regularizer = regularizers.l2(0.01), activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1000, kernel_regularizer = regularizers.l2(0.01), activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10,  activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the Keral model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 2.0437 - accuracy: 0.8560 - val_loss: 0.6959 - val_accuracy: 0.9207\n",
      "Epoch 2/3\n",
      "469/469 [==============================] - 16s 34ms/step - loss: 0.7666 - accuracy: 0.8909 - val_loss: 0.6629 - val_accuracy: 0.9223\n",
      "Epoch 3/3\n",
      "469/469 [==============================] - 19s 40ms/step - loss: 0.7316 - accuracy: 0.8980 - val_loss: 0.6170 - val_accuracy: 0.9341\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train, batch_size=128,\n",
    "                 epochs=3, verbose=1,\n",
    "                 validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 13ms/step - loss: 0.6170 - accuracy: 0.9341\n",
      "Test loss 0.6170, accuracy 93.41%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test loss {:.4f}, accuracy {:.2f}%\".format(score[0], score[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-training Quantization\n",
    "\n",
    "These methods performs quantization on already trained models. We don't need to develop a new model architecture, but we can start with an existing floating point model and quantize it to obtain a fixed point quantized model with almost no accuracy loss, without needing to re-train the model. The **model size reduction** that we can obtain is a **factor of 4**, with negligible accuracy loss.  \n",
    "\n",
    "We gain also other benefit: reducing the precision at which this data is stored leads to less working memory needed; most processors allow for faster processing of 8-bit data; the reduction in the amount of data movement can have a significant impact also on the power consumption.\n",
    "\n",
    "All the factors above translate into **faster inference**, with a typical **speedup of 2-3x** due to the reduced precision for both memory accesses and computations. \n",
    "\n",
    "The cost is usually o modest decrease in accuracy.\n",
    "\n",
    "There are some ways in which post-training quantization can be done. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Range Quantization\n",
    "\n",
    "It is the simplest form of post-training quantization which statically quantizes the weights from floating point to 8-bits of precision and dynamically quantizes the activations at inference. This means that the activations are always stored in float 32, however, they are converted to 8-bit integers while processing and back to floating point after the processing is done. \n",
    "\n",
    "Following are the steps to perform post-training quantization on our keras model. First of all, we save the  model into a single HDF5 file which will contain the architecture, weights, training configuration (loss, optimizer) and the state of the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#Save the entire model in model.h5 file\n",
    "\n",
    "model.save(\"models/model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we can use the TensorFlow Lite converter with and without quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 15:17:12.896617: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-05-18 15:17:15.429669: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2022-05-18 15:17:15.429686: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2022-05-18 15:17:15.430376: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/np/rkxq5s9x6lzdnx_24x0j3jx80000gn/T/tmpfs_xjl_n\n",
      "2022-05-18 15:17:15.432332: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2022-05-18 15:17:15.432350: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /var/folders/np/rkxq5s9x6lzdnx_24x0j3jx80000gn/T/tmpfs_xjl_n\n",
      "2022-05-18 15:17:15.440059: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-05-18 15:17:15.488541: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /var/folders/np/rkxq5s9x6lzdnx_24x0j3jx80000gn/T/tmpfs_xjl_n\n",
      "2022-05-18 15:17:15.503273: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 72899 microseconds.\n",
      "2022-05-18 15:17:15.527463: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7185856"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('models/model.h5')\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "open(\"models/converted_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 15:18:09.450010: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2022-05-18 15:18:09.450027: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2022-05-18 15:18:09.450150: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/np/rkxq5s9x6lzdnx_24x0j3jx80000gn/T/tmpnoq001y5\n",
      "2022-05-18 15:18:09.452015: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2022-05-18 15:18:09.452030: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /var/folders/np/rkxq5s9x6lzdnx_24x0j3jx80000gn/T/tmpnoq001y5\n",
      "2022-05-18 15:18:09.464238: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-05-18 15:18:09.516164: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /var/folders/np/rkxq5s9x6lzdnx_24x0j3jx80000gn/T/tmpnoq001y5\n",
      "2022-05-18 15:18:09.531550: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 81401 microseconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1804024"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "open(\"models/converted_quant_model.tflite\", \"wb\").write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the reduction in size of the model. Notice, it is approximately 4x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model in Mb: 6.85296630859375\n",
      "Quantized model in Mb: 1.7204513549804688\n",
      "Compression ratio: 3.98323747355911\n"
     ]
    }
   ],
   "source": [
    "print(\"Float model in Mb:\", os.path.getsize('models/converted_model.tflite') / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize('models/converted_quant_model.tflite') / float(2**20))\n",
    "print(\"Compression ratio:\", os.path.getsize('models/converted_model.tflite')/os.path.getsize('models/converted_quant_model.tflite'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate the accuracy of converted model on the same test set. In order to make inference with TF Lite, we need to initialize the interpreter, load it with the model, allocate the tensor and get the input and output tensors, preprocess data by reading it into a tensor,  make the inference on the input tensor using the interpreter by invoking it, and finally obtain the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.39\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"models/converted_quant_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape']\n",
    "acc=0\n",
    "for i in range(len(x_test)):\n",
    "    input_data = x_test[i].reshape(input_shape)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    if(np.argmax(output_data) == np.argmax(y_test[i])):\n",
    "        acc+=1\n",
    "acc = acc/len(x_test)\n",
    "print(acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decide to quantizes the float32 weights into float16, resulting in 2x size reduction. It is generally used with specialized hardware like GPUs which can directly work with float16, thereby providing some speed-up over float32 operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static Quantization\n",
    "\n",
    "Static quantization pre-computes the scales and zero points also for all the activation tensors. Therefore, the overhead of computing the scales and zero points is eliminated. The activation tensors could be stored as integer tensors in the memory without having to be converted from floating point tensors. The way to determine the scales and zero points for all the activation tensors is simple. Given a floating point neural network, we would just have to run the neural network using some representative unlabeled data, collect the distribution statistics for all the activation layers. Then we could use the distribution statistics to compute the scales and zero points using the mathematical equations we have described earlier in the article.\n",
    "\n",
    "During inference, because all the computations were conducted seamlessly using integer ops, the inference performance is really fast. The only short-coming is that we have to prepare representative unlabeled data. If the data is not representative, the scales and zero points computed might not reflect the true scenario during inference, and the inference accuracy will be harmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, _ = tf.keras.datasets.mnist.load_data()\n",
    "images = tf.cast(mnist_train[0], tf.float32) / 255.0\n",
    "mnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\n",
    "\n",
    "def representative_data_gen():\n",
    "  for input_value in mnist_ds.take(100):\n",
    "    yield [input_value]\n",
    "    \n",
    "converter.representative_dataset = representative_data_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization Aware Training\n",
    "\n",
    "Quantization introduces information loss and therefore the inference accuracies from the quantized integer models are inevitably lower than that from the floating point models. Such information loss is due to that the floating points after quantization and de-quantization is not exactly recoverable. Mathematically, it means:\n",
    "\n",
    "$\\begin{align}\n",
    "x = f_d(f_q(x,s_x,z_x),s_x,z_x) + \\Delta_x\n",
    "\\end{align}$\n",
    "\n",
    "If $\\Delta_x=0$ the inference accuracies from the quantized integer models would be exactly the same as the inference accuracies from the floating point models. Unfortunately, it is not. The idea of quantization aware training is to ask the neural networks to take the effect of such information loss into account during training. Therefore, during inference, the model will have less sacrifice to the inference accuracies. Therefore, in quantization aware training, we added a quantization and a de-quantization layer for each of the tensors. Mathematically, it means\n",
    "\n",
    "$\\begin{align}\n",
    "\\hat{x} = f_d(f_q(x,s_x,z_x),s_x,z_x) = s_x(\\text{clip}(\\text{round}(\\frac{x}{s_x}+z_x),\\alpha_q,\\beta_q)-z_x)\n",
    "\\end{align}$\n",
    "\n",
    "where the scale and zero point could be collected using the same method of static quantization. All the data types for the quantized tensors are still floating point tensors and the training is supposed to be done normally as if the quantization and de-quantization layers were not existed. \n",
    "\n",
    "The major problem of quantization aware training is that such quantization and de-quantization layers are not differentiable. In practice, the [straight through estimation (STE)](https://arxiv.org/abs/1308.3432) derivative approximation works well for quantization aware training. Essentially, it treats the quantization and de-quantization function as if it were identity function in the clipping range $[\\alpha, \\beta]$ and constant function outside the clipping range. Therefore, the resulting derivatives are 1 in the clippings range  and 0 outside the clipping range \n",
    "Mathematically, it could be described as:\n",
    "\n",
    "$\\begin{align}\n",
    "\\frac{\\partial \\hat{x}}{\\partial y}= \\Bigg\\{\\begin{matrix} 1 \\enspace  \\text{if} \\enspace \\alpha \\lt x \\lt  \\beta  \\\\ 0 \\enspace \\enspace \\enspace \\enspace \\enspace \\enspace \\enspace \\text{else} \\end{matrix}  \n",
    "\\end{align}$\n",
    "\n",
    "<img src=\"./images/quantization-aware-training.png\" width=\"500\"> \n",
    "\n",
    "Quantization Aware Training typically provides higher accuracies as compared to post-training quantization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-model-optimization in /Users/riccardo.berta/opt/anaconda3/lib/python3.9/site-packages (0.7.2)\n",
      "Requirement already satisfied: six~=1.10 in /Users/riccardo.berta/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-model-optimization) (1.16.0)\n",
      "Requirement already satisfied: numpy~=1.14 in /Users/riccardo.berta/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-model-optimization) (1.21.2)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /Users/riccardo.berta/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-model-optimization) (0.1.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantizing the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer (QuantizeLay  (None, 784)              3         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " quant_dropout (QuantizeWrap  (None, 784)              1         \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " quant_dense (QuantizeWrappe  (None, 1000)             785005    \n",
      " rV2)                                                            \n",
      "                                                                 \n",
      " quant_dropout_1 (QuantizeWr  (None, 1000)             1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_1 (QuantizeWrap  (None, 1000)             1001005   \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " quant_dropout_2 (QuantizeWr  (None, 1000)             1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_2 (QuantizeWrap  (None, 10)               10015     \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,796,031\n",
      "Trainable params: 1,796,010\n",
      "Non-trainable params: 21\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "q_aware_model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6661 - accuracy: 0.9061 - val_loss: 0.5452 - val_accuracy: 0.9429\n",
      "Epoch 2/3\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.6582 - accuracy: 0.9080 - val_loss: 0.5364 - val_accuracy: 0.9486\n",
      "Epoch 3/3\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6576 - accuracy: 0.9062 - val_loss: 0.5332 - val_accuracy: 0.9447\n"
     ]
    }
   ],
   "source": [
    "hist = q_aware_model.fit(x_train, y_train, batch_size=128,\n",
    "                         epochs=3, verbose=1,\n",
    "                         validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6170 - accuracy: 0.9341\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.5332 - accuracy: 0.9447\n",
      "Baseline test accuracy: 93.40999722480774\n",
      "Quant test accuracy: 94.47000026702881\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "_, q_aware_model_accuracy = q_aware_model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy*100)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantizing some layers\n",
    "\n",
    "This second method gives us more granularity in order to avoid quantization of layers that are more important for accuracy (for example, later layers instead of the first layers). In that case we need to annotate the layer we want to quantize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate = tfmot.quantization.keras.quantize_annotate_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_annotated = Sequential()\n",
    "model_annotated.add(layers.Dropout(0.2,input_shape=(784,)))\n",
    "model_annotated.add(layers.Dense(1000, kernel_regularizer = regularizers.l2(0.01), activation='relu'))\n",
    "model_annotated.add(layers.Dropout(0.5))\n",
    "model_annotated.add(annotate(layers.Dense(1000, kernel_regularizer = regularizers.l2(0.01), activation='relu')))\n",
    "model_annotated.add(layers.Dropout(0.5))\n",
    "model_annotated.add(layers.Dense(10,  activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dropout_3 (Dropout)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1000)              785000    \n",
      "                                                                 \n",
      " quant_dropout_4 (QuantizeWr  (None, 1000)             1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_4 (QuantizeWrap  (None, 1000)             1001005   \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                10010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,796,016\n",
      "Trainable params: 1,796,010\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "quant_aware_model_annotated = tfmot.quantization.keras.quantize_apply(model_annotated)\n",
    "quant_aware_model_annotated.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_aware_model_annotated.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                                    optimizer='adam', \n",
    "                                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "469/469 [==============================] - 14s 29ms/step - loss: 2.0394 - accuracy: 0.8553 - val_loss: 0.6911 - val_accuracy: 0.9208\n",
      "Epoch 2/3\n",
      "469/469 [==============================] - 15s 31ms/step - loss: 0.7628 - accuracy: 0.8906 - val_loss: 0.6396 - val_accuracy: 0.9300\n",
      "Epoch 3/3\n",
      "469/469 [==============================] - 16s 34ms/step - loss: 0.7289 - accuracy: 0.8982 - val_loss: 0.6096 - val_accuracy: 0.9372\n"
     ]
    }
   ],
   "source": [
    "hist = quant_aware_model_annotated.fit(x_train, y_train, batch_size=128,\n",
    "                                       epochs=3, verbose=1, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6170 - accuracy: 0.9341\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.5332 - accuracy: 0.9447\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6096 - accuracy: 0.9372\n",
      "Baseline test accuracy: 93.40999722480774\n",
      "Quant test accuracy: 94.47000026702881\n",
      "Quant annotated test accuracy: 93.72000098228455\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "_, q_aware_model_accuracy = q_aware_model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "_, quant_aware_model_accuracy = quant_aware_model_annotated.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy*100)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy*100)\n",
    "print('Quant annotated test accuracy:', quant_aware_model_accuracy*100)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "math_differential_calculus",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "8aeb84091b1f1fb8d8b9efbf1e96a552fa0144c39bfbc7f744113ad2216f701d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
